{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "from google.oauth2.service_account import Credentials\n",
    "import gspread\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_name_checked = ['Docked', 'Arrived', 'Counting', 'QC', 'Labeling',\n",
    "                    'Received', 'Putaway', 'Picking', 'Packing', 'AWB', 'RTS',\n",
    "                    'RT_picking', 'RT_putaway', 'Cyclecount', 'Print']  # , 'Testing']  # 新增新的種類\n",
    "\n",
    "cat_name = ['Docked', 'Arrived', 'Counting', 'QC', 'Labeling',\n",
    "            'Received', 'Putaway', 'Putaway_4floor', 'Picking', 'Packing',\n",
    "            'AWB', 'RTS', 'RT_picking', 'RT_picking_4floor', 'RT_putaway',\n",
    "            'RT_putaway_4floor', 'Cyclecount', 'Cyclecount_4floor', 'Print']  # , 'Testing']  # 新增新的種類\n",
    "\n",
    "\n",
    "type_dic = {\n",
    "    '碼頭收發': 'Docked',\n",
    "    '收貨': 'Arrived',\n",
    "    '進貨計數': 'Counting',\n",
    "    '品管': 'QC',\n",
    "    '貼標': 'Labeling',\n",
    "    '貴重驗收': 'Received',\n",
    "    '箱賣': 'Received',\n",
    "    '小驗': 'Received',\n",
    "    '大驗': 'Received',\n",
    "    '上架基架': 'Putaway',\n",
    "    '上架棧板': 'Putaway',\n",
    "    '上架基架_四樓': 'Putaway_4floor',\n",
    "    '上架棧板_四樓': 'Putaway_4floor',\n",
    "    '揀貨': 'Picking',\n",
    "    '包裝': 'Packing',\n",
    "    '出貨': 'AWB',\n",
    "    '退貨出貨': 'RTS',\n",
    "    '退貨包裝': 'RTS',\n",
    "    '退貨揀貨': 'RTS',\n",
    "    '移庫揀貨': 'RT_picking',\n",
    "    '移庫上架': 'RT_putaway',\n",
    "    '移庫揀貨_四樓': 'RT_picking_4floor',\n",
    "    '移庫上架_四樓': 'RT_putaway_4floor',\n",
    "    '盤點系統盤': 'Cyclecount',\n",
    "    '盤點系統盤_四樓': 'Cyclecount_4floor',\n",
    "    '印標': 'Print'\n",
    "    # '出貨5S': 'Testing'  # 直接加上新的種類即可\n",
    "}\n",
    "\n",
    "productivity_varable = {\n",
    "    'DL%': 1,\n",
    "    'DL % threshold': 0.6,\n",
    "    'Docked': 75,\n",
    "    'Arrived': 125,\n",
    "    'QC': 4638,\n",
    "    'Labeling': 850,\n",
    "    'Received': 800,\n",
    "    'Putaway': 65,\n",
    "    'Putaway_4floor': 65,\n",
    "    'Picking': 114,\n",
    "    'Packing': 143,\n",
    "    'Counting': 1000,\n",
    "    'AWB': 720,\n",
    "    'RTS': 300,\n",
    "    'RT_picking': 726,\n",
    "    'RT_putaway': 726,\n",
    "    'RT_picking_4floor': 726,\n",
    "    'RT_putaway_4floor': 726,\n",
    "    'Cyclecount': 850,\n",
    "    'Cyclecount_4floor': 850,\n",
    "    'Print': 200  # 20210716待確認\n",
    "    # 'Testing': 20  # 新增計算IPH指標\n",
    "}\n",
    "\n",
    "team_prod_dict = {\n",
    "    'Picking': '出貨控場',\n",
    "    'Packing': '出貨控場',\n",
    "    'AWB': '出貨控場',\n",
    "    'Arrived': '進貨控場',\n",
    "    'Counting': '進貨控場',\n",
    "    'QC': '進貨控場',\n",
    "    'Labeling': '進貨控場',\n",
    "    'Received': '進貨控場',\n",
    "    'Docked': '進貨控場',\n",
    "    'Print': '進貨控場',\n",
    "    'RT_picking': '移庫控場',\n",
    "    'RT_putaway': '移庫控場',\n",
    "    'RT_picking_4floor': '移庫控場_四樓',\n",
    "    'RT_putaway_4floor': '移庫控場_四樓',\n",
    "    'RTS': np.nan,\n",
    "    'Putaway': '移庫控場',\n",
    "    'Putaway_4floor': '移庫控場_四樓',\n",
    "    'Cyclecount': '盤點控場',\n",
    "    'Cyclecount_4floor': '盤點控場_四樓'\n",
    "    # 'Testing': '測試控場'  # 新增種類的控場\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gdoc_information():\n",
    "    def __init__(self):\n",
    "        self.SCOPES = \"\"\n",
    "        self.SAMPLE_SPREADSHEET_ID = \"\"\n",
    "        self.SAMPLE_RANGE_NAME = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_schema = gdoc_information()\n",
    "ppl_schema.SCOPES = 'https://docs.google.com/spreadsheets/d/1fKqmL3VS1aDjdeJR_MqLQwu9mdEjf_Ci8PV1QCp-M6Q'  # 不用每個月更改\n",
    "ppl_schema.SAMPLE_RANGE_NAME = '通訊錄'  # 抓整張工作表，之後再選要的欄位\n",
    "\n",
    "tag_gdoc = gdoc_information()\n",
    "tag_gdoc.SCOPES = 'https://docs.google.com/spreadsheets/d/1GUvKT8BxFsHLwgM2Jptmkpc0pIZMetoVtIUIet5UxB8/edit'  # 每個月要改網址\n",
    "\n",
    "docked_gdoc = gdoc_information()\n",
    "docked_gdoc.SCOPES = 'https://docs.google.com/spreadsheets/d/1eDn98UQJuJRKN-8IaQo6MDlOMCCV4HZE2Q8iSA6oXeE/edit'  # 不用每個月更改\n",
    "docked_gdoc.SAMPLE_RANGE_NAME = \"Sheet1\"\n",
    "\n",
    "print_gdoc = gdoc_information()\n",
    "print_gdoc.SCOPES = 'https://docs.google.com/spreadsheets/d/1uBRnzC3oNGKKjWt8kHRzYBj75ZDe-G9YW6wzPsR6fxY/edit'  # 不用每個月更改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "creds = Credentials.from_service_account_file(\"credentials.json\", scopes=scope)\n",
    "gs = gspread.authorize(creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 1: 匯入打卡資料並進行前處理\n",
    "def read_punch_file(path, revise_station_name, type_dic):\n",
    "    '''\n",
    "    讀入站點打卡_for-attendance資料，進行整理\n",
    "    ----------------\n",
    "    Input:\n",
    "    1. path: 站點打卡路徑(punch_file_name)\n",
    "    2. revise_station_name: 要將站點進行參照的表格\n",
    "    2. type_dic: 字典，用於將站點打卡的中文站點轉換為英文\n",
    "    '''\n",
    "    punch_station = pd.read_excel(revise_station_name)  # 參照revise_station的工作表\n",
    "    punch_station['lookup'] = punch_station['Unnamed: 1']\\\n",
    "        .str.cat(punch_station['function_name'], sep=', ')\\\n",
    "        .str.cat(punch_station['function_role'], sep=', ')  # 將Unnamed, function_name, function_role三個欄位合再一起，作為參照\n",
    "\n",
    "    punch_raw_df = pd.read_excel(path)\n",
    "    punch_raw_df = (punch_raw_df[~pd.isnull(punch_raw_df['name'])])  # 只保留有名字的打卡記錄\n",
    "    punch_raw_df = punch_raw_df[(punch_raw_df[\"date\"] >= start_day) & (punch_raw_df[\"date\"] - timedelta(days = 1) <= end_day)]\n",
    "    punch_raw_df.drop_duplicates(inplace=True)  # 移除重複項目\n",
    "    \n",
    "    punch_raw_df['ID'] = punch_raw_df['ID'].str.lower()  # 將打卡員編轉為小寫，以利後續參照\n",
    "    punch_raw_df['type'] = punch_raw_df['function'].map(type_dic)  # 新增type，為type_dic的工作種類\n",
    "    punch_raw_df['type'] = punch_raw_df['type'].astype('str').replace('nan', np.nan)  # 將類別轉為字串格式，缺失值(不算Productivity的項目)為np.nan\n",
    "    punch_raw_df['hour'] = punch_raw_df['min'] / 60  # 新增小時欄位\n",
    "\n",
    "    punch_raw_df['lookup'] = punch_raw_df['Unnamed: 7']\\\n",
    "        .str.cat(punch_raw_df['function_name'], sep=', ')\\\n",
    "        .str.cat(punch_raw_df['function_role'], sep=', ')  # 將Unnamed, function_name, function_role三個欄位合再一起，作為參照\n",
    "    punch_raw_df.rename(columns={'date': 'create_date', 'ID': 'operator'})\n",
    "    punch_raw_df = punch_raw_df.merge(punch_station[['lookup', 'revised station']], on='lookup')\\\n",
    "                               .drop('lookup', axis=1)  # 參照完就把參照欄位lookup丟棄\n",
    "    punch_raw_df.sort_values('created_time', inplace=True)  # 之後merge_asof需要排序\n",
    "    punch_raw_df.reset_index(drop=True, inplace=True)\n",
    "    return punch_raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 2: 匯入人力資料並進行前處理\n",
    "def read_human_data():\n",
    "    '''\n",
    "    抓取「人力資料_schema」資料，並轉成後續需要的字典\n",
    "    1. name_id_dic: 姓名(key)與員編(value)\n",
    "    2. id_name_dic: 員編(key)與姓名(value)\n",
    "    3. pda_name_dic: PDA帳號(key)與姓名(value)\n",
    "    4. pda_id_dic: PDA帳號(key)與員編(value)\n",
    "    '''\n",
    "    \n",
    "    human_gsheet = gs.open_by_url(ppl_schema.SCOPES).worksheet(ppl_schema.SAMPLE_RANGE_NAME)\n",
    "    human_df = pd.DataFrame(human_gsheet.get_all_records(), columns=[\"WMS帳號\", \"公司\", \"PDA帳號\", \"worker_name\"])\n",
    "    human_df.columns = ['員編', '公司', 'PDA帳號', 'worker_name']\n",
    "    \n",
    "    id_name_dic = {str(x).lower(): y for x, y in zip(human_df['員編'], human_df['worker_name'])}\n",
    "    name_id_dic = {}\n",
    "    for key, value in id_name_dic.items():\n",
    "        if value not in name_id_dic.keys():\n",
    "            name_id_dic[value] = key\n",
    "    pda_name_dic = {str(x): y for x, y in zip(human_df['PDA帳號'], human_df['worker_name'])}\n",
    "    pda_id_dic = {str(x): str(y).lower() for x, y in zip(human_df['員編'], human_df['PDA帳號'])}\n",
    "    return name_id_dic, id_name_dic, pda_name_dic, pda_id_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 3: 將IB_production新增貼標、收發、印標資料\n",
    "def add_data_in_inb(time2):\n",
    "#     '''\n",
    "#     1. 新增貼標到 inb_pics_file_path (IB_production) (2021/05)\n",
    "#     2. 新增收發到 inb_pics_file_path (IB_production) (2021/05)\n",
    "#     3. 新增印標到 inb_pics_file_path (IB_production) (2021/07 新增)\n",
    "#     output: 更新inb_pics_file_path\n",
    "#     '''\n",
    "    # 3-1 抓Google Sheet「人力資料schema」，存為ppl_schema_df(DataFrame)\n",
    "    ppl_schema_gsheet = gs.open_by_url(ppl_schema.SCOPES).worksheet(ppl_schema.SAMPLE_RANGE_NAME)\n",
    "    ppl_schema_df = pd.DataFrame(ppl_schema_gsheet.get_all_records(), columns=[\"WMS帳號\", \"PDA帳號\"])\n",
    "    ppl_schema_df.columns = ['員編', '貼標ID']\n",
    "    ppl_schema_df['貼標ID'] = ppl_schema_df['貼標ID'].astype(\"str\")\n",
    "    ppl_schema_df.dropna(inplace=True)\n",
    "    time3_1 = time.time()\n",
    "    print('Checkpoint 3-1 人力資料_schema SUCCEED    Spend {:.2f} seconds'.format(time3_1 - time2))\n",
    "\n",
    "#     # 3-2 抓取貼標資料，在get_gdoc.get_tag_data中匯出excel，並存為tag_summary\n",
    "    \n",
    "    tag_df = pd.DataFrame()\n",
    "    tag_gsheet = gs.open_by_url(tag_gdoc.SCOPES)\n",
    "    \n",
    "    for day in pd.date_range(start=start_day,end=end_day):\n",
    "        df = get_everyday_tag_data(day.strftime(\"%Y-%m-%d\"), tag_gsheet)\n",
    "        tag_df = pd.concat([tag_df, df])\n",
    "        time.sleep(3)\n",
    "    \n",
    "    tag_df = tag_df[[\"版標流水號\", \"貼標開始\", \"貼標人數(人)\", \"貼標ID\"]]\n",
    "    \n",
    "    wms_label_df = pd.read_csv(wms_label).rename(columns={\"_col0\": \"date\"}) \n",
    "    wms_label_df[\"date\"] = pd.to_datetime(wms_label_df[\"date\"], errors='coerce')\n",
    "    wms_label_df = wms_label_df[(wms_label_df[\"date\"] >= start_day) & (wms_label_df[\"date\"] - timedelta(days = 1) <= end_day)][['tracking_id', 'batch_qty']] # 抓取每個流水號每個batch有多少數量\n",
    "\n",
    "    tag_df = pd.merge(tag_df, wms_label_df, left_on='版標流水號', right_on='tracking_id') # 將每個貼標有多少個batch結合\n",
    "    tag_df[\"貼標ID\"] = tag_df[\"貼標ID\"].astype('str')\n",
    "    tag_df['貼標人數(人)'] = tag_df['貼標人數(人)'].astype(\"int\")\n",
    "    tag_df['貼標開始'] = pd.to_datetime(tag_df['貼標開始'], errors='coerce')\n",
    "    tag_df['員工作業PCS'] = tag_df['batch_qty'] / tag_df['貼標人數(人)']\n",
    "    tag_summary = tag_df.groupby(['貼標開始', '貼標ID']).sum()\n",
    "    tag_summary = tag_summary.reset_index()\n",
    "    tag_summary = tag_summary.merge(ppl_schema_df, left_on='貼標ID', right_on='貼標ID', how='left')  # 得到貼標的員編\n",
    "    tag_summary = tag_summary[tag_summary['員編'].notnull()]\n",
    "\n",
    "    tag_summary['type'] = 'Labeling'\n",
    "    tag_summary['box'] = 0  # 其他種類才用到box，貼標資料皆為0\n",
    "    tag_summary['orders'] = 0  # 其他種類才用到orders，貼標資料皆為0\n",
    "    tag_summary = tag_summary[['員編', 'type', '員工作業PCS', 'box', 'orders', '貼標開始']]\n",
    "    tag_summary.columns = ['operator', 'type', 'total_pcs', 'box', 'orders', 'inbound_date']  # 合併資料統一要這幾個欄位\n",
    "    tag_summary['inbound_date'] = pd.to_datetime(tag_summary['inbound_date'], errors='coerce')\n",
    "    print(tag_summary.head())\n",
    "    time3_2 = time.time()\n",
    "    print('Checkpoint 3-2 tag_summary SUCCEED        Spend {:.2f} seconds'.format(time3_2 - time3_1))\n",
    "    \n",
    "    # 3-3 抓取新增收發，並匯出excel，並存為docked_summary\n",
    "    docked_gsheet = gs.open_by_url(docked_gdoc.SCOPES).worksheet(docked_gdoc.SAMPLE_RANGE_NAME)\n",
    "    docked_df = pd.DataFrame(docked_gsheet.get_all_records())\n",
    "    docked_df['收發時間'] = pd.to_datetime(docked_df['收發時間'], errors='coerce')\n",
    "    docked_df.dropna(subset=[\"收發時間\"], axis=0, inplace=True)\n",
    "    docked_df[\"DATE\"] = docked_df[\"收發時間\"].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    docked_df[\"HOUR\"] = docked_df[\"收發時間\"].apply(lambda x: x.hour)\n",
    "    \n",
    "    docked_df = docked_df[(docked_df[\"收發時間\"] >= start_day) & (docked_df[\"收發時間\"] - timedelta(days = 1) <= end_day)]\n",
    "    \n",
    "    docked_path = 'tmp_output/docked_raw_{}.xlsx'.format(month)\n",
    "    docked_df.to_excel(docked_path, index=False)\n",
    "\n",
    "    docked_df.columns = ['員編', 'INbound ID', '國碼', '是否拒收', '狀態', '備註', 'Cancel後新單', 'QTY', '收發時間', 'DATE', 'HOUR']\n",
    "    docked_df['員編'] = docked_df['員編'].astype('str')\n",
    "    docked_summary = docked_df.groupby(['收發時間', '員編'])['INbound ID'].count()\n",
    "    docked_summary = docked_summary.reset_index()\n",
    "\n",
    "    # Mapping 人力資料 schema 五碼變SP\n",
    "    docked_summary = docked_summary.rename(columns={\"員編\": \"五碼\"})\n",
    "    docked_summary = docked_summary.merge(ppl_schema_df, left_on='五碼', right_on='貼標ID', how='left')\n",
    "    docked_summary['type'] = 'Docked'\n",
    "    docked_summary['box'] = 0  # 其他種類才用到box，收發資料皆為0\n",
    "    docked_summary['total_pcs'] = 0  # 其他種類才用到orders，收發資料皆為0\n",
    "    docked_summary = docked_summary[['員編', 'type', 'total_pcs', 'box', 'INbound ID', '收發時間']]\n",
    "    docked_summary.columns = ['operator', 'type', 'total_pcs', 'box', 'orders', 'inbound_date']  # 合併資料統一要這幾個欄位\n",
    "    \n",
    "    print(docked_summary.head())\n",
    "    time3_3 = time.time()\n",
    "#     print('Checkpoint 3-3 docked_summary SUCCEED     Spend {:.2f} seconds'.format(time3_3 - time3_2))\n",
    "\n",
    "    # 檔案4. print_summary: 如果有檔案，直接讀取過去檔案；反之則執行processing.take_month_data取得資料\n",
    "    print_df = pd.DataFrame()\n",
    "    print_gsheet = gs.open_by_url(print_gdoc.SCOPES)\n",
    "    for day in pd.date_range(start=start_day,end=end_day):\n",
    "        df = get_everyday_print_data(day.strftime(\"%Y-%m-%d\"), print_gsheet)\n",
    "        print_df = pd.concat([print_df, df])\n",
    "        time.sleep(3)\n",
    "\n",
    "    print_df = print_df[[\"印標人員\", \"DATE\"]]\n",
    "    print_df['印標人員'] = print_df['印標人員'].apply(lambda x : str(x).replace(\"x\", \"0\").replace(\"X\", \"0\"))\n",
    "    print_df['印標人員'] = print_df['印標人員'].astype(\"str\")\n",
    "    print_summary = print_df.merge(ppl_schema_df, left_on='印標人員', right_on='貼標ID', how='left')\n",
    "    print_summary = print_summary[print_summary['員編'].notnull()]\n",
    "    print_summary['type'] = 'Print'\n",
    "    print_summary['box'] = 0  # 其他種類才用到box，印標資料皆為0\n",
    "    print_summary['total_pcs'] = 0  # 其他種類才用到orders，印標資料皆為0\n",
    "    print_summary['orders'] = 1  # 每個orders = 1\n",
    "    print_summary = print_summary[['員編', 'type', 'total_pcs', 'box', 'orders', 'DATE']]\n",
    "    print_summary.columns = ['operator', 'type', 'total_pcs', 'box', 'orders', 'inbound_date']  # 合併資料統一要這幾個欄位\n",
    "    print(print_summary.head())\n",
    "    time3_4 = time.time()\n",
    "    print('Checkpoint 3-4 print_df SUCCEED           Spend {:.2f} seconds'.format(time3_4 - time3_3))\n",
    "\n",
    "    ib_df = pd.read_excel(inb_pics_file_path)\n",
    "    ib_df = ib_df[(ib_df[\"inbound_date\"] >= start_day) & (ib_df[\"inbound_date\"] - timedelta(days = 1) <= end_day)]\n",
    "\n",
    "    ib_df = ib_df.append(tag_summary)\n",
    "    ib_df = ib_df.append(docked_summary)\n",
    "    ib_df = ib_df.append(print_summary)\n",
    "\n",
    "    ib_df.to_excel(inb_pics_file_path_new, index=False)\n",
    "    time3_5 = time.time()\n",
    "    print('Checkpoint 3-5 add to excel SUCCEED       Spend {:.2f} seconds'.format(time3_5 - time3_4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 4: 輸入資料格式統一\n",
    "# Checkpoint 4-1: IB_production\n",
    "def read_ibs(inb_pics_file_path_new, id_name_dic):\n",
    "    '''\n",
    "    read inbound PICS 的資料 (excel)\n",
    "    input:\n",
    "    1. inb_pics_file_path_new\n",
    "    2. id_name_dic: 名字對應到 id\n",
    "    '''\n",
    "    inb_pic_df = pd.read_excel(inb_pics_file_path_new, parse_dates=['inbound_date'])\n",
    "    inb_pic_df = inb_pic_df.rename(columns={'inbound_date': 'create_date'})\n",
    "    inb_pic_df = inb_pic_df[inb_pic_df['operator'].notnull()]  # 排除 operator 為空的列\n",
    "    inb_pic_df['operator'] = inb_pic_df['operator'].str.lower()  # 員編轉小寫\n",
    "    inb_pic_df['name'] = inb_pic_df['operator'].map(id_name_dic)  # 利用 id 轉名字\n",
    "    inb_pic_df = inb_pic_df[['name', 'operator', 'type', 'create_date', 'total_pcs', 'box', 'orders']]\n",
    "    inb_pic_df = inb_pic_df.rename(columns={'total_pcs': 'pcs', 'create_date': 'create_time'})\n",
    "    return inb_pic_df[['name', 'operator', 'type', 'create_time', 'pcs', 'box', 'orders']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 4-2: OB_production\n",
    "def read_obs(ob_pics_file_path, id_name_dic, pda_id_dic):\n",
    "    '''\n",
    "    read oubound / inv PICS 的資料 (excel)\n",
    "    input:\n",
    "    1. path_name : PICS 資料連結\n",
    "    2. name_id_dic: 名字對應到 id\n",
    "    read oubound / inv PICS 的資料 (csv)\n",
    "    因為資料欄位名稱不一樣，所以才要分開讀\n",
    "    '''\n",
    "    ob_pic_df = pd.read_excel(ob_pics_file_path, parse_dates=['create_time'])\n",
    "    ob_pic_df = ob_pic_df[(ob_pic_df[\"create_time\"] >= start_day) & (ob_pic_df[\"create_time\"] - timedelta(days = 1) <= end_day)]\n",
    "    \n",
    "    ob_pic_df['workers'] = ob_pic_df['workers'].str.lower().astype('str')\n",
    "    ob_pic_df['type'] = ob_pic_df['type'].map({'1_picking': 'Picking', '3_packing': 'Packing', '4_awb': 'AWB'})\n",
    "\n",
    "    def get_operator(worker):\n",
    "        if 'sp' not in worker and worker in pda_id_dic:\n",
    "            return pda_id_dic[worker]\n",
    "        else:\n",
    "            return worker\n",
    "    ob_pic_df['operator'] = ob_pic_df['workers'].apply(get_operator)  # 如果workers是員編就輸出員編，是PDA帳號就轉成員編\n",
    "    ob_pic_df['name'] = ob_pic_df['operator'].map(id_name_dic)\n",
    "    ob_pic_df['box'] = 0\n",
    "    ob_pic_df['orders'] = 0\n",
    "    return ob_pic_df[['name', 'operator', 'type', 'create_time', 'pcs', 'box', 'orders']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 4-3: INV_production\n",
    "def read_inv(inv_pics_file_path, id_name_dic):\n",
    "    '''\n",
    "    read oubound / inv PICS 的資料 (excel)\n",
    "    input:\n",
    "    1. path_name: PICS 資料連結\n",
    "    2. name_id_dic: 名字對應到 id\n",
    "    read oubound / inv PICS 的資料 (csv)\n",
    "    因為資料欄位名稱不一樣，所以才要分開讀\n",
    "    '''\n",
    "\n",
    "    inv_pic_df = pd.read_excel(inv_pics_file_path, parse_dates=['create_date'])\n",
    "    inv_pic_df = inv_pic_df[(inv_pic_df[\"create_date\"] >= start_day) & (inv_pic_df[\"create_date\"] - timedelta(days = 1) <= end_day)]\n",
    "    \n",
    "    inv_pic_df = inv_pic_df[inv_pic_df['operator'].notnull()]  # 排除 operator 為空的列\n",
    "    inv_pic_df['operator'] = inv_pic_df['operator'].str.lower()  # 員編轉小寫\n",
    "    inv_pic_df['type'] = np.where(inv_pic_df['type'] == 'Cycle_count', 'Cyclecount', inv_pic_df['type'])  # type 字串轉換\n",
    "    inv_pic_df['name'] = inv_pic_df['operator'].map(id_name_dic)  # 利用 id 轉名字\n",
    "    inv_pic_df['box'] = 0\n",
    "    inv_pic_df['orders'] = 0\n",
    "    inv_pic_df = inv_pic_df.rename(columns={'create_date': 'create_time'})\n",
    "    return inv_pic_df[['name', 'operator', 'type', 'create_time', 'pcs', 'box', 'orders']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 4-4: 將IB_production、OB_production、INV_production資料合併，得到whole_df\n",
    "def get_whole_df(ib_df, inv_df, ob_df):\n",
    "    '''\n",
    "    將ib_df、inv_df、ob_df合併\n",
    "    input: ib_df, inv_df, ob_df\n",
    "    output: 合併後的資料whole_df\n",
    "    '''\n",
    "    whole_df = pd.concat([ib_df, inv_df, ob_df])\n",
    "    whole_df['create_time'] = pd.to_datetime(whole_df['create_time'], errors='coerce')  # 轉不了日期就跳過\n",
    "\n",
    "    whole_df.dropna(how='any', inplace=True)\n",
    "    whole_df = whole_df[whole_df['create_time'].dt.date != datetime.date(1899, 12, 30)]\n",
    "    whole_df.sort_values(['create_time'], inplace=True)\n",
    "    # 'total_pcs'直接列出之後計算IPH的Productivity，'Arrived', 'Docked' 使用orders計算，'Putaway'使用box計算，其他皆使用pcs計算\n",
    "    whole_df['total_pcs'] = np.where(\n",
    "        whole_df['type'].isin(['Arrived', 'Docked', 'Print']), whole_df['orders'],\n",
    "        np.where(whole_df['type'] == 'Putaway', whole_df['box'], whole_df['pcs']))\n",
    "    return whole_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 5-1: 將whole_df、punch_df合併，得到merge_df\n",
    "def get_merge_df(whole_df, punch_df):\n",
    "    '''\n",
    "    將whole_df、punch_df合併，並判斷whole_df的create time是否在punch_df打卡的時段\n",
    "    input: whole_df, punch_df\n",
    "    output: merge_df\n",
    "    '''\n",
    "    \n",
    "    whole_df.sort_values('create_time', inplace=True)\n",
    "    punch_df.sort_values('created_time', inplace=True)\n",
    "\n",
    "    merge_df = pd.merge_asof(\n",
    "        whole_df, punch_df.drop('name', axis=1),\n",
    "        left_on=\"create_time\", right_on=\"created_time\",\n",
    "        left_by=\"operator\", right_by=\"ID\", direction='backward')\n",
    "    merge_df = merge_df.rename(columns={'type_x': 'type', 'type_y': 'punch_type'})\n",
    "    merge_df['punch_type'] = merge_df['punch_type'].astype('str')\n",
    "    merge_df['merge_type'] = merge_df['punch_type'].str.replace('_4floor', '')\n",
    "    merge_df['valid_time'] = (merge_df['create_time'] >= merge_df['created_time']) & (merge_df['create_time'] <= merge_df['end_time'])\n",
    "    merge_df['valid_type'] = (merge_df['type'].values == merge_df['merge_type'].values) & merge_df['valid_time']\n",
    "    merge_df['Check Result'] = np.where(merge_df['valid_time'].values,\n",
    "                                        np.where(merge_df['valid_type'].values, 'Correct', 'Wrong Station'),\n",
    "                                        'No data')\n",
    "    merge_df['created_time'] = np.where(merge_df['Check Result'].values == 'No data',\n",
    "                                        np.datetime64('NaT'),\n",
    "                                        merge_df['created_time'].values)\n",
    "    merge_df['end_time'] = np.where(merge_df['Check Result'].values == 'No data',\n",
    "                                    np.datetime64('NaT'),\n",
    "                                    merge_df['end_time'].values)\n",
    "    merge_df['print_label'] = np.where(merge_df['Check Result'].values == 'Wrong Station',\n",
    "                                       merge_df['revised station'].values,\n",
    "                                       np.nan)\n",
    "    return merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 5-2: 將merge_df依各種工作種類合併(位於calculate_score.py)\n",
    "def get_valid_csv(merge_df, cat_name_checked):\n",
    "    '''\n",
    "    將5-1 merge_df的結果依不同cat_type分別儲存成csv檔\n",
    "    input:\n",
    "    1. merge_df\n",
    "    2. cat_name_checked: 目前不分樓層\n",
    "    '''\n",
    "    valid_whole_df = merge_df[['name', 'operator', 'type', 'create_time', 'pcs', 'box', 'orders', 'total_pcs',\n",
    "                               'Check Result', 'created_time', 'end_time', 'print_label']]\n",
    "    for cat in cat_name_checked:\n",
    "        cat_df = valid_whole_df[valid_whole_df['type'] == cat]\n",
    "        cat_df.to_csv('Output/incentive_checked/{}.csv'.format(cat), encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 6: 計算productivity_agent\n",
    "def get_prod_agent_score(cat_name, productivity_varable, whole_df, punch_df, agent_output_path):\n",
    "    '''\n",
    "    計算Agent的Productivity Score\n",
    "    input:\n",
    "    1. cat_name: 工作type的list\n",
    "    2. productivity_varable: 每種工作type的IPH績效\n",
    "    3. whole_df: 結合IB、OB、INV的資料\n",
    "    4. punch_df: 整理後打卡記錄表\n",
    "    output: 計算績效的DataFrame\n",
    "    '''\n",
    "    # 1. punch_ids人員資料\n",
    "    punch_ids = punch_df[['ID', 'name', 'role', 'class', 'group']].drop_duplicates().set_index('ID').sort_index()\n",
    "    merge_df = get_merge_df(whole_df, punch_df)\n",
    "    merge_df['type'] = np.where(merge_df['punch_type'].str.contains('_4floor'),\n",
    "                                merge_df['punch_type'], merge_df['type'])\n",
    "    punch_df['DL'] = punch_df['type'].notnull()  # 有沒有對應的cat_type\n",
    "\n",
    "    # 2. DL_count工作時數及有在cat_type的時間比例\n",
    "    DL_count = pd.crosstab(punch_df['ID'], punch_df['DL'], values=punch_df['hour'], aggfunc=np.sum)\n",
    "    DL_count.fillna(0, inplace=True)\n",
    "    DL_count.columns = ['not_DL', 'DL']\n",
    "    DL_count['total'] = DL_count['DL'].values + DL_count['not_DL'].values\n",
    "    DL_count['DL%'] = DL_count['DL'].values / DL_count['total'].values\n",
    "    DL_count = DL_count[['DL', 'not_DL',  # 有cat_type的工作時數、沒有cat_type的工作時數\n",
    "                         'total', 'DL%']]  # 總時數、有cat_type的工作時數的比例\n",
    "\n",
    "    # 3. pcs_count完成數量資訊\n",
    "    pcs_count = pd.crosstab(merge_df['operator'], merge_df['type'], values=merge_df['total_pcs'], aggfunc=np.sum).add_prefix('PCS_')\n",
    "    for cat in cat_name:\n",
    "        if 'PCS_{}'.format(cat) not in pcs_count.columns:\n",
    "            print('whole_df 無 {} 資料'.format(cat))\n",
    "            pcs_count['PCS_{}'.format(cat)] = 0\n",
    "    pcs_count = pcs_count[['PCS_{}'.format(cat) for cat in cat_name]]\n",
    "\n",
    "    # 4. hour_count工作時數資訊\n",
    "    hour_count = pd.crosstab(punch_df['ID'], punch_df['type'], values=punch_df['hour'], aggfunc=np.sum).add_prefix('Hour_')\n",
    "    for cat in cat_name:\n",
    "        if 'Hour_{}'.format(cat) not in hour_count.columns:\n",
    "            hour_count['Hour_{}'.format(cat)] = 0\n",
    "    \n",
    "    hour_count = hour_count[['Hour_{}'.format(cat) for cat in cat_name]]\n",
    "    \n",
    "    # productivity_table合併punch_ids, DL_count, pcs_count, hour_count\n",
    "    productivity_table = punch_ids.merge(DL_count, left_index=True, right_index=True, how='left')\\\n",
    "                                  .merge(pcs_count, left_index=True, right_index=True, how='left')\\\n",
    "                                  .merge(hour_count, left_index=True, right_index=True, how='left')\n",
    "    # 計算IPH分數: Hour = 0就是0，不然就是PCS/Hour\n",
    "    for cat in cat_name:\n",
    "        productivity_table['IPH_{}'.format(cat)] = np.where(productivity_table['Hour_{}'.format(cat)] == 0, 0,\n",
    "                                                            productivity_table['PCS_{}'.format(cat)] / productivity_table['Hour_{}'.format(cat)])\n",
    "    for cat in cat_name:\n",
    "        productivity_table['HR%_{}'.format(cat)] = productivity_table['Hour_{}'.format(cat)] / productivity_table['DL']\n",
    "\n",
    "    # IPH與目標的差距\n",
    "    for cat in cat_name:\n",
    "        productivity_table[cat] = productivity_table['IPH_{}'.format(cat)] / productivity_varable[cat]\n",
    "\n",
    "    # 計算Productivity Score\n",
    "    scores = pd.DataFrame()\n",
    "    for cat in cat_name:\n",
    "        scores[cat] = productivity_table[cat].values * productivity_table['HR%_{}'.format(cat)].values\n",
    "    scores['Productivity Score'] = scores.sum(axis=1)\n",
    "    scores.index = productivity_table.index\n",
    "\n",
    "    # 把Productivity Score合併至productivity_table\n",
    "    productivity_table = productivity_table.merge(scores[['Productivity Score']], left_index=True, right_index=True)\n",
    "    productivity_table.fillna(0, inplace=True)\n",
    "    productivity_table.reset_index(inplace=True)\n",
    "    productivity_table.to_excel(agent_output_path, index=False)\n",
    "    return productivity_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 7: 計算productivity_TL\n",
    "def get_prod_TL_score(productivity_varable, team_prod_dict, whole_df, punch_df, tl_output_path):\n",
    "    '''\n",
    "    計算Team Lead的Productivity Score\n",
    "    Team Lead: 只要打卡紀錄function_name出現過MGMT即視為Team Lead，但只計算每次打卡期間超過30分鐘的打卡\n",
    "    input:\n",
    "    1. productivity_varable: s每種工作type的IPH績效\n",
    "    3. whole_df: 結合IB、OB、INV的資料\n",
    "    4. punch_df: 整理後打卡記錄表\n",
    "    output: 計算績效的DataFrame\n",
    "    '''\n",
    "    merge_df = get_merge_df(whole_df, punch_df)\n",
    "    merge_df['type'] = np.where(merge_df['punch_type'].str.contains('_4floor'), merge_df['punch_type'], merge_df['type'])\n",
    "    iph = merge_df.groupby(['ID', 'type', 'created_time', 'end_time', 'hour'])['total_pcs'].agg(np.sum).reset_index()\\\n",
    "                  .rename(columns={'sum': 'total_pcs'})\n",
    "    iph['function'] = iph['type'].map(team_prod_dict)\n",
    "\n",
    "    def prod_ratio_calculate(iph, function, start, end):\n",
    "        iph_ckeck = iph[(iph['function'].values == function) &\n",
    "                        (iph['created_time'].values <= end) &  # 在該段時間內該cat_type的站點打卡\n",
    "                        (iph['end_time'].values >= start)]\\\n",
    "                        .groupby(['ID', 'type'])[['hour', 'total_pcs']].agg(np.sum).reset_index()\n",
    "        iph_ckeck['hour'] = np.where(iph_ckeck['hour'].values == 0, 0.008333, iph_ckeck['hour'].values)  # 0分鐘的資料在此算30秒(0.008333小時)\n",
    "        iph_ckeck['iph'] = iph_ckeck['total_pcs'].values / iph_ckeck['hour'].values\n",
    "        iph_ckeck['meet_goal'] = np.where(iph_ckeck['iph'].values >= iph_ckeck['type'].map(productivity_varable), 1, 0)\n",
    "        return pd.Series([np.sum(iph_ckeck['meet_goal']),\n",
    "                          iph_ckeck.shape[0]])\n",
    "    \n",
    "    # 1. team_df：以每次打卡記錄計算\n",
    "    team_df = punch_df[(punch_df['function_name'] == 'MGMT') & (punch_df['min'] >= 30)]  # 只要function_name有出現過MGMT就算Team Lead，只計算控場超過30分鐘的資料\n",
    "    team_df[['arrive_thres', 'count']] = team_df.apply(lambda row: prod_ratio_calculate(iph, row['function'], row['created_time'], row['end_time']), axis=1)\n",
    "    team_df['prod_hour_ratio'] = np.where(team_df['count'] == 0, 0, team_df['arrive_thres'].values / team_df['count'].values)\n",
    "\n",
    "    # 2. team_df_day：以每天打卡記錄計算\n",
    "    team_df_day = team_df.groupby(['ID', 'name', 'date', 'function'])[['hour', 'arrive_thres', 'count']].agg(np.sum).reset_index()\n",
    "    team_df_day['prod_day_ratio'] = np.where(team_df_day['count'] == 0, 0, team_df_day['arrive_thres'].values / team_df_day['count'].values)\n",
    "\n",
    "    # 3. productivity_tl：該月每個team lead負責控場天數及平均達標率（若一天有兩種控場，算兩天）\n",
    "    productivity_tl = team_df_day.groupby(['ID', 'name'])['prod_day_ratio'].agg(['count', np.mean]).reset_index()\n",
    "    productivity_tl = productivity_tl.rename(columns={'count': 'days_on_duty', 'mean': 'TL_produtivity_score'})\n",
    "\n",
    "    # 4. productivity_team_function：該月每個team lead每天控場達標率\n",
    "    productivity_team_function = pd.crosstab(\n",
    "        [team_df_day['ID'], team_df_day['name'], team_df_day['function']],\n",
    "        team_df_day['date'],\n",
    "        values=team_df_day['prod_day_ratio'], aggfunc='mean')\n",
    "    productivity_team_function['date_on_duty'] = productivity_team_function.count(axis=1)\n",
    "    productivity_team_function.reset_index(inplace=True)\n",
    "\n",
    "    with pd.ExcelWriter(tl_output_path) as writer:\n",
    "        team_df.to_excel(writer, sheet_name='team_df', index=False, encoding=\"utf_8_sig\")\n",
    "        team_df_day.to_excel(writer, sheet_name='team_df_day', index=False, encoding=\"utf_8_sig\")\n",
    "        productivity_tl.to_excel(writer, sheet_name='productivity_tl', index=False, encoding=\"utf_8_sig\")\n",
    "        productivity_team_function.to_excel(writer, sheet_name='productivity_team_function', index=False, encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_everyday_print_data(day, print_gsheet):\n",
    "    day_obj = datetime.datetime.strptime(day, '%Y-%m-%d')\n",
    "    SAMPLE_RANGE_NAME = \"{}/{}\".format(day_obj.month, day_obj.day) # 抓幾月幾號的表\n",
    "    cols = ['是否印標', '印標人員', 'Tracking ID', '尾碼', 'SKU ID', 'DATE']\n",
    "    try:\n",
    "        print_df = pd.DataFrame(print_gsheet.worksheet(SAMPLE_RANGE_NAME).get_all_values())\n",
    "        print_df = print_df.rename(columns={\n",
    "            print_df.columns[0]: cols[0],\n",
    "            print_df.columns[1]: cols[1],\n",
    "            print_df.columns[2]: cols[2],\n",
    "            print_df.columns[3]: cols[3], \n",
    "            print_df.columns[4]: cols[4], \n",
    "            print_df.columns[17]: cols[5], \n",
    "        })\n",
    "        print_df.filter(items=cols)\n",
    "        print_df = print_df[(print_df['是否印標'] == 'V') & (print_df['SKU ID'] != '不用印')]\n",
    "        print_df.drop_duplicates(subset=['Tracking ID'], keep='first', inplace=True)\n",
    "        print('get {} data'.format(day))\n",
    "    except:  # 該天無印標資料\n",
    "        print_df = pd.DataFrame(columns=cols)\n",
    "        print('g-doc no data: {}'.format(day))\n",
    "    \n",
    "    return print_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_everyday_tag_data(day, tag_gsheet):\n",
    "    SAMPLE_RANGE_NAME = day.replace(\"-\", \"\")  # 抓幾月幾號的表，例如2021-06-01就抓20210601\n",
    "    columns = ['版標流水號', '貼標開始', '貼標結束', '是否結束', '花費時間', '貼標人數(人)', '貼標ID']\n",
    "    try:\n",
    "        tag_df = pd.DataFrame(tag_gsheet.worksheet(SAMPLE_RANGE_NAME).get_all_records())\n",
    "        tag_df.columns = columns\n",
    "        tag_df.dropna(axis=0, inplace=True)\n",
    "        tag_df['operator'] = tag_df['貼標ID'].astype(\"str\").str.lower()\n",
    "        tag_df['貼標人數(人)'] = tag_df['貼標人數(人)'].astype('int')\n",
    "        print('get {} data'.format(day))\n",
    "    except:  # 該天無印標資料\n",
    "        print('g-doc no data: {}'.format(SAMPLE_RANGE_NAME))\n",
    "        tag_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    return tag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 8: 將merge_df進行validation，產出 valid_whole_df\n",
    "def get_valid_whole_df(merge_df):\n",
    "    '''\n",
    "    將5-1的 merge_df 按照以下規則進行篩選：\n",
    "    1. 打卡時間位於 punch starting time and punch ending time\n",
    "    2. RT_putaway 和 Putaway 為 1. 之例外 \n",
    "    '''\n",
    "\n",
    "    merge_df[\"keep\"] = merge_df[\"valid_type\"]\n",
    "\n",
    "    for index, row in merge_df.iterrows():\n",
    "        if row[\"type\"] == \"Putaway\" and row[\"merge_type\"] == \"RT_putaway\":\n",
    "            merge_df.loc[index, \"keep\"] = True\n",
    "        elif row[\"type\"] == \"RT_putaway\" and row[\"merge_type\"] == \"Putaway\":\n",
    "            merge_df.loc[index, \"keep\"] = True\n",
    "\n",
    "    valid_whole_df = merge_df.copy()\n",
    "    valid_whole_df = valid_whole_df[valid_whole_df[\"keep\"]]\n",
    "    valid_whole_df = valid_whole_df[['name', 'operator', 'type', 'create_time', 'pcs', 'box', 'orders', 'total_pcs']]\n",
    "    valid_whole_df.to_csv(\"tmp_output/valid_whole_df_{}.csv\".format(month_fullname), encoding=\"utf_8_sig\", index=False)\n",
    "\n",
    "    return valid_whole_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_foler(month_fullname):\n",
    "\n",
    "    if not os.path.exists(\"Output/\"):\n",
    "        os.makedirs(\"Output/\")\n",
    "    if not os.path.exists(\"tmp_output/\"):\n",
    "        os.makedirs(\"tmp_output/\")\n",
    "    if not os.path.exists(\"Output/incentive_checked\"):\n",
    "        os.makedirs(\"Output/incentive_checked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 1 read_punch_file SUCCEED      Spend 5.57 seconds\n",
      "Checkpoint 2 read_human_datas SUCCEED     Spend 2.30 seconds\n",
      "Checkpoint 3-1 人力資料_schema SUCCEED    Spend 2.21 seconds\n",
      "g-doc no data: 2022-04-01\n",
      "get 2022-04-02 data\n",
      "get 2022-04-03 data\n",
      "g-doc no data: 2022-04-04\n",
      "g-doc no data: 2022-04-05\n",
      "get 2022-04-06 data\n",
      "get 2022-04-07 data\n",
      "get 2022-04-08 data\n",
      "get 2022-04-09 data\n",
      "g-doc no data: 2022-04-10\n",
      "get 2022-04-11 data\n",
      "get 2022-04-12 data\n",
      "get 2022-04-13 data\n",
      "get 2022-04-14 data\n",
      "get 2022-04-15 data\n",
      "get 2022-04-16 data\n",
      "get 2022-04-17 data\n",
      "get 2022-04-18 data\n",
      "get 2022-04-19 data\n",
      "get 2022-04-20 data\n",
      "get 2022-04-21 data\n",
      "get 2022-04-22 data\n",
      "get 2022-04-23 data\n",
      "get 2022-04-24 data\n",
      "get 2022-04-25 data\n",
      "get 2022-04-26 data\n",
      "get 2022-04-27 data\n",
      "get 2022-04-28 data\n",
      "get 2022-04-29 data\n",
      "get 2022-04-30 data\n",
      "  operator   type  total_pcs  box  orders        inbound_date\n",
      "0   SP2949  Print          0    0       1  2022-04-02 8:08:47\n",
      "1   SP2949  Print          0    0       1  2022-04-02 8:14:12\n",
      "2   SP2949  Print          0    0       1  2022-04-02 8:13:46\n",
      "3   SP2949  Print          0    0       1  2022-04-02 8:13:18\n",
      "4   SP2949  Print          0    0       1  2022-04-02 8:12:49\n",
      "Checkpoint 3-4 print_df SUCCEED           Spend 131.95 seconds\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tag_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-bf856867e941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Checkpoint 2 read_human_datas SUCCEED     Spend {:.2f} seconds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0madd_data_in_inb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mtime3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Checkpoint 3 add_data_in_inb SUCCEED      Spend {:.2f} seconds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-159-d26921d0c48b>\u001b[0m in \u001b[0;36madd_data_in_inb\u001b[0;34m(time2)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mib_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mib_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mib_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"inbound_date\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mstart_day\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mib_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"inbound_date\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mend_day\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mib_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mib_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0mib_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mib_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocked_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mib_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mib_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tag_summary' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Time\n",
    "    month = \"2022-04\"\n",
    "    start_day, end_day = \"2022-04-01\", \"2022-04-30\"\n",
    "    month_first_day = datetime.datetime.strptime(month, \"%Y-%m\")\n",
    "    month_num = str(month_first_day.month)  # 得到str月份\n",
    "    month_shortname = month_first_day.strftime(\"%b\")  # e.g. Jul, Jun\n",
    "    month_fullname = month_first_day.strftime(\"%B\")  # e.g. July, June\n",
    "    \n",
    "    # Input Files\n",
    "    punch_file_name = 'Input/punch_for-attendance_{}.xlsx'.format(month_fullname)\n",
    "    revise_station_name = 'Input/revise_station.xlsx'\n",
    "    inb_pics_file_path = 'Input/IB_production_{}.xlsx'.format(month_fullname)\n",
    "    inb_pics_file_path_new = 'Input/IB_production_{}_new.xlsx'.format(month_fullname)  # IB_production增加印標、收發、貼標後會儲存在此，並做為之後計算的input\n",
    "    ob_pics_file_path = 'Input/OB_production_{}.xlsx'.format(month_fullname)\n",
    "    inv_pics_file_path = 'Input/INV_production_{}.xlsx'.format(month_fullname)\n",
    "    wms_label = 'Input/WMS_label.csv'\n",
    "    \n",
    "    # Output Files\n",
    "    output_foler(month_fullname)\n",
    "    tl_output_path = \"Output/productivity_TL_{}.xlsx\".format(month_fullname)\n",
    "    agent_output_path = \"Output/productivity_agent_{}.xlsx\".format(month_fullname)\n",
    "    tl_valid_output_path = \"Output/productivity_TL_{}_valid.xlsx\".format(month_fullname)\n",
    "    agent_valid_output_path = \"Output/productivity_agent_{}_valid.xlsx\".format(month_fullname)\n",
    "    \n",
    "    \n",
    "    time0 = time.time()\n",
    "    punch_df = read_punch_file(punch_file_name, revise_station_name, type_dic)\n",
    "    punch_df.to_csv('tmp_output/punch_df_{}.csv'.format(month_fullname), index=False, encoding=\"utf_8_sig\")\n",
    "    punch_df.dropna(subset=['created_time', 'end_time'], axis=0, inplace=True)\n",
    "    time1 = time.time()\n",
    "    print('Checkpoint 1 read_punch_file SUCCEED      Spend {:.2f} seconds'.format(time1 - time0))\n",
    "    \n",
    "    name_id_dic, id_name_dic, pda_name_dic, pda_id_dic = read_human_data()\n",
    "    time2 = time.time()\n",
    "    print('Checkpoint 2 read_human_datas SUCCEED     Spend {:.2f} seconds'.format(time2 - time1))\n",
    "\n",
    "    add_data_in_inb(time2)\n",
    "    time3 = time.time()\n",
    "    print('Checkpoint 3 add_data_in_inb SUCCEED      Spend {:.2f} seconds'.format(time3 - time2))\n",
    "    \n",
    "    ib_df = read_ibs(inb_pics_file_path_new, id_name_dic)\n",
    "    time4_1 = time.time()\n",
    "    print('Checkpoint 4-1 ib_df SUCCEED              Spend {:.2f} seconds'.format(time4_1 - time3))\n",
    "\n",
    "    ob_df = read_obs(ob_pics_file_path, id_name_dic, pda_id_dic)\n",
    "    time4_2 = time.time()\n",
    "    print('Checkpoint 4-2 ob_df SUCCEED              Spend {:.2f} seconds'.format(time4_2 - time4_1)) \n",
    "    \n",
    "    inv_df = read_inv(inv_pics_file_path, id_name_dic)\n",
    "    time4_3 = time.time()\n",
    "    print('Checkpoint 4-3 inv_df SUCCEED             Spend {:.2f} seconds'.format(time4_3 - time4_2))\n",
    "\n",
    "    whole_df = get_whole_df(ib_df, inv_df, ob_df)\n",
    "    whole_df.to_csv('tmp_output/whole_df_{}.csv'.format(month_fullname), index=False, encoding=\"utf_8_sig\")\n",
    "    time4_4 = time.time()\n",
    "    print('Checkpoint 4-4 whole_df SUCCEED           Spend {:.2f} seconds'.format(time4_4 - time4_3))\n",
    "\n",
    "    time4 = time.time()\n",
    "    print('Checkpoint 4 whole_df SUCCEED             Spend {:.2f} seconds'.format(time4 - time3))\n",
    "\n",
    "    merge_df = get_merge_df(whole_df, punch_df)\n",
    "#     merge_df.to_csv(\"tmp_output/merge_df_{}.csv\".format(month_fullname), index=False, encoding=\"utf_8_sig\")\n",
    "    time5_1 = time.time()\n",
    "    print('Checkpoint 5-1 get_merge_df SUCCEED       Spend {:.2f} seconds'.format(time5_1 - time4))\n",
    "\n",
    "    get_valid_csv(merge_df, cat_name_checked)\n",
    "    time5_2 = time.time()\n",
    "    print('Checkpoint 5-2 get_valid_csv SUCCEED      Spend {:.2f} seconds'.format(time5_2 - time5_1))\n",
    "    time5 = time.time()\n",
    "    print('Checkpoint 5 SUCCEED   Spend {:.2f} seconds'.format(time5 - time4))\n",
    "\n",
    "\n",
    "    get_prod_agent_score(cat_name, productivity_varable, whole_df, punch_df, agent_output_path)\n",
    "    time6 = time.time()\n",
    "    print('Checkpoint 6 productivity_agent SUCCEED   Spend {:.2f} seconds'.format(time6 - time5))\n",
    "\n",
    "    get_prod_TL_score(productivity_varable, team_prod_dict, whole_df, punch_df, tl_output_path)\n",
    "    time7 = time.time()\n",
    "    print('Checkpoint 7 productivity_TL SUCCEED      Spend {:.2f} seconds'.format(time7 - time6))\n",
    "\n",
    "    valid_whole_df = get_valid_whole_df(merge_df)\n",
    "\n",
    "    valid_whole_df.dropna(axis=0, inplace=True)\n",
    "    time8 = time.time()\n",
    "    print('Checkpoint 8 get_valid_whole_df SUCCEED      Spend {:.2f} seconds'.format(time8 - time7))\n",
    "\n",
    "    get_prod_agent_score(cat_name, productivity_varable, valid_whole_df, punch_df, agent_valid_output_path)\n",
    "    time9 = time.time()\n",
    "    print('Checkpoint 9 productivity_valid_agent SUCCEED     Spend {:.2f} seconds'.format(time9 - time8))\n",
    "\n",
    "    get_prod_TL_score(productivity_varable, team_prod_dict, valid_whole_df, punch_df, tl_valid_output_path)\n",
    "    time10 = time.time()\n",
    "    print('Checkpoint 10 productivity_valid_TL SUCCEED        Spend {:.2f} seconds'.format(time10 - time9))\n",
    "\n",
    "    print('計算完成 共花費{:.2f}秒'.format(time10 - time0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
